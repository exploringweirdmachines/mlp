{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SECOND VIDEO STARTS HERE\n",
    "https://www.youtube.com/watch?v=TCH_1BHY58I"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Now let's try to predict more than one character"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next character in a sequence quickly blow up, the table size of counts table grows exponentially, if we only take a single character at a time, that's 27 posibilities, but if we take 2 characters and try to predict the next one, then 27 x 27, there's 729 posibilities, then if we take into context three chars, then suddenly we have 20000 posibilities, so there's way just too many rows of the matrix N, and doesn't work very well."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['emma', 'olivia', 'ava', 'isabella', 'sophia', 'charlotte', 'mia', 'amelia']"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "words = open(\"names.txt\", 'r').read().splitlines()\n",
    "words[:8]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "32033"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{1: 'a',\n",
       " 2: 'b',\n",
       " 3: 'c',\n",
       " 4: 'd',\n",
       " 5: 'e',\n",
       " 6: 'f',\n",
       " 7: 'g',\n",
       " 8: 'h',\n",
       " 9: 'i',\n",
       " 10: 'j',\n",
       " 11: 'k',\n",
       " 12: 'l',\n",
       " 13: 'm',\n",
       " 14: 'n',\n",
       " 15: 'o',\n",
       " 16: 'p',\n",
       " 17: 'q',\n",
       " 18: 'r',\n",
       " 19: 's',\n",
       " 20: 't',\n",
       " 21: 'u',\n",
       " 22: 'v',\n",
       " 23: 'w',\n",
       " 24: 'x',\n",
       " 25: 'y',\n",
       " 26: 'z',\n",
       " 0: '.'}"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# build the vocabulary of characters and mappings to/from integers\n",
    "\n",
    "chars = sorted(list(set(''.join(words))))\n",
    "stoi = {s:i+1 for i,s in enumerate(chars)}\n",
    "stoi['.'] = 0\n",
    "itos = {i:s for s,i in stoi.items()}\n",
    "itos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "emma\n",
      "... ----> e\n",
      "..e ----> m\n",
      ".em ----> m\n",
      "emm ----> a\n",
      "mma ----> .\n",
      "olivia\n",
      "... ----> o\n",
      "..o ----> l\n",
      ".ol ----> i\n",
      "oli ----> v\n",
      "liv ----> i\n",
      "ivi ----> a\n",
      "via ----> .\n",
      "ava\n",
      "... ----> a\n",
      "..a ----> v\n",
      ".av ----> a\n",
      "ava ----> .\n",
      "isabella\n",
      "... ----> i\n",
      "..i ----> s\n",
      ".is ----> a\n",
      "isa ----> b\n",
      "sab ----> e\n",
      "abe ----> l\n",
      "bel ----> l\n",
      "ell ----> a\n",
      "lla ----> .\n",
      "sophia\n",
      "... ----> s\n",
      "..s ----> o\n",
      ".so ----> p\n",
      "sop ----> h\n",
      "oph ----> i\n",
      "phi ----> a\n",
      "hia ----> .\n"
     ]
    }
   ],
   "source": [
    "# build the dataset\n",
    "\n",
    "block_size = 3 # context length: how many characters do we take to predict the next one\n",
    "X, Y = [], []\n",
    "\n",
    "for w in words[:5]:\n",
    "    print(w)\n",
    "    context = [0] * block_size\n",
    "\n",
    "    for ch in w + '.':\n",
    "        ix = stoi[ch]\n",
    "        X.append(context)\n",
    "        Y.append(ix)\n",
    "        print(''.join(itos[i] for i in context), '---->', itos[ix])\n",
    "        context = context[1:] + [ix] # crop and append\n",
    "\n",
    "X = torch.tensor(X)\n",
    "Y = torch.tensor(Y)\n",
    "# this is kind of like a rolling window of context\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([32, 3]), torch.int64, torch.Size([32]), torch.int64)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X.shape, X.dtype, Y.shape, Y.dtype"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_408443/871842550.py:3: UserWarning: Named tensors and all their associated APIs are an experimental feature and subject to change. Please do not use them for anything important until they are released as stable. (Triggered internally at ../c10/core/TensorImpl.h:1900.)\n",
      "  C = torch.randn((27, 2), names=[\"characters\", \"embeddings_2D\"])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([[-1.5386e+00,  1.8487e+00],\n",
       "        [-6.7712e-01,  1.9750e+00],\n",
       "        [ 6.4264e-01,  7.5298e-01],\n",
       "        [ 4.1627e-01, -1.3786e+00],\n",
       "        [-4.4313e-01,  7.5561e-01],\n",
       "        [ 1.6538e+00, -1.6568e+00],\n",
       "        [-2.1692e+00,  1.2204e+00],\n",
       "        [ 4.9455e-02,  1.2173e-03],\n",
       "        [ 7.0645e-01, -6.2030e-01],\n",
       "        [ 1.7211e+00,  1.4553e+00],\n",
       "        [-4.0264e-01,  7.8527e-01],\n",
       "        [ 1.6993e+00,  1.3837e+00],\n",
       "        [ 7.3711e-01,  4.8597e-01],\n",
       "        [ 2.2373e-02, -1.1911e-01],\n",
       "        [ 4.6678e-01,  1.4867e+00],\n",
       "        [ 1.7984e-03,  1.6052e+00],\n",
       "        [ 9.0456e-01,  6.3669e-01],\n",
       "        [ 3.4110e-01, -1.7836e+00],\n",
       "        [-1.0568e+00, -1.0574e-01],\n",
       "        [-4.7952e-01, -1.0606e+00],\n",
       "        [-1.5863e+00, -5.8871e-01],\n",
       "        [-2.9061e-02,  1.1486e+00],\n",
       "        [ 2.8026e-01, -1.0524e+00],\n",
       "        [-1.2587e+00,  4.4348e-01],\n",
       "        [-7.2958e-01, -1.6767e+00],\n",
       "        [-3.1377e-01,  7.8485e-01],\n",
       "        [ 2.8619e-01, -2.7903e-01]], names=('characters', 'embeddings_2D'))"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Let's cram the 27 possible characters into a two-dimensional space\n",
    "\n",
    "C = torch.randn((27, 2), names=[\"characters\", \"embeddings_2D\"])\n",
    "C"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([ 1.6538, -1.6568], names=('embeddings_2D',))"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# let's encode number 5, plucking out the sixth entry\n",
    "C[5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([ 1.6538, -1.6568], names=('embeddings_2D',))"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "F.one_hot(torch.tensor(5), num_classes=27).float() @ C"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 1.6538e+00, -1.6568e+00],\n",
       "        [-2.1692e+00,  1.2204e+00],\n",
       "        [ 4.9455e-02,  1.2173e-03]])"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "C = C.rename(None)\n",
    "C[[5, 6, 7]].rename(None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 1.6538e+00, -1.6568e+00],\n",
       "        [-2.1692e+00,  1.2204e+00],\n",
       "        [ 4.9455e-02,  1.2173e-03]])"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "C[torch.tensor([5, 6, 7])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 1.6538e+00, -1.6568e+00],\n",
       "        [-2.1692e+00,  1.2204e+00],\n",
       "        [ 4.9455e-02,  1.2173e-03],\n",
       "        [ 4.9455e-02,  1.2173e-03],\n",
       "        [ 4.9455e-02,  1.2173e-03],\n",
       "        [ 4.9455e-02,  1.2173e-03]])"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "C[torch.tensor([5, 6, 7, 7, 7, 7])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([32, 3, 2])"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# torch indexing is awesome\n",
    "C[X].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0,  0,  0],\n",
       "        [ 0,  0,  5],\n",
       "        [ 0,  5, 13],\n",
       "        [ 5, 13, 13],\n",
       "        [13, 13,  1],\n",
       "        [ 0,  0,  0],\n",
       "        [ 0,  0, 15],\n",
       "        [ 0, 15, 12],\n",
       "        [15, 12,  9],\n",
       "        [12,  9, 22],\n",
       "        [ 9, 22,  9],\n",
       "        [22,  9,  1],\n",
       "        [ 0,  0,  0],\n",
       "        [ 0,  0,  1],\n",
       "        [ 0,  1, 22],\n",
       "        [ 1, 22,  1],\n",
       "        [ 0,  0,  0],\n",
       "        [ 0,  0,  9],\n",
       "        [ 0,  9, 19],\n",
       "        [ 9, 19,  1],\n",
       "        [19,  1,  2],\n",
       "        [ 1,  2,  5],\n",
       "        [ 2,  5, 12],\n",
       "        [ 5, 12, 12],\n",
       "        [12, 12,  1],\n",
       "        [ 0,  0,  0],\n",
       "        [ 0,  0, 19],\n",
       "        [ 0, 19, 15],\n",
       "        [19, 15, 16],\n",
       "        [15, 16,  8],\n",
       "        [16,  8,  9],\n",
       "        [ 8,  9,  1]])"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([ 5, 13, 13,  1,  0, 15, 12,  9, 22,  9,  1,  0,  1, 22,  1,  0,  9, 19,\n",
       "         1,  2,  5, 12, 12,  1,  0, 19, 15, 16,  8,  9,  1,  0])"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([-0.6771,  1.9750])"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "C[X][13, 2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([-0.6771,  1.9750])"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "C[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-1.5386e+00,  1.8487e+00],\n",
       "        [-6.7712e-01,  1.9750e+00],\n",
       "        [ 6.4264e-01,  7.5298e-01],\n",
       "        [ 4.1627e-01, -1.3786e+00],\n",
       "        [-4.4313e-01,  7.5561e-01],\n",
       "        [ 1.6538e+00, -1.6568e+00],\n",
       "        [-2.1692e+00,  1.2204e+00],\n",
       "        [ 4.9455e-02,  1.2173e-03],\n",
       "        [ 7.0645e-01, -6.2030e-01],\n",
       "        [ 1.7211e+00,  1.4553e+00],\n",
       "        [-4.0264e-01,  7.8527e-01],\n",
       "        [ 1.6993e+00,  1.3837e+00],\n",
       "        [ 7.3711e-01,  4.8597e-01],\n",
       "        [ 2.2373e-02, -1.1911e-01],\n",
       "        [ 4.6678e-01,  1.4867e+00],\n",
       "        [ 1.7984e-03,  1.6052e+00],\n",
       "        [ 9.0456e-01,  6.3669e-01],\n",
       "        [ 3.4110e-01, -1.7836e+00],\n",
       "        [-1.0568e+00, -1.0574e-01],\n",
       "        [-4.7952e-01, -1.0606e+00],\n",
       "        [-1.5863e+00, -5.8871e-01],\n",
       "        [-2.9061e-02,  1.1486e+00],\n",
       "        [ 2.8026e-01, -1.0524e+00],\n",
       "        [-1.2587e+00,  4.4348e-01],\n",
       "        [-7.2958e-01, -1.6767e+00],\n",
       "        [-3.1377e-01,  7.8485e-01],\n",
       "        [ 2.8619e-01, -2.7903e-01]])"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "C"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([32, 3, 2])"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# lookup table C\n",
    "embeddings = C[X]\n",
    "embeddings.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[-1.5386e+00,  1.8487e+00],\n",
       "         [-1.5386e+00,  1.8487e+00],\n",
       "         [-1.5386e+00,  1.8487e+00]],\n",
       "\n",
       "        [[-1.5386e+00,  1.8487e+00],\n",
       "         [-1.5386e+00,  1.8487e+00],\n",
       "         [ 1.6538e+00, -1.6568e+00]],\n",
       "\n",
       "        [[-1.5386e+00,  1.8487e+00],\n",
       "         [ 1.6538e+00, -1.6568e+00],\n",
       "         [ 2.2373e-02, -1.1911e-01]],\n",
       "\n",
       "        [[ 1.6538e+00, -1.6568e+00],\n",
       "         [ 2.2373e-02, -1.1911e-01],\n",
       "         [ 2.2373e-02, -1.1911e-01]],\n",
       "\n",
       "        [[ 2.2373e-02, -1.1911e-01],\n",
       "         [ 2.2373e-02, -1.1911e-01],\n",
       "         [-6.7712e-01,  1.9750e+00]],\n",
       "\n",
       "        [[-1.5386e+00,  1.8487e+00],\n",
       "         [-1.5386e+00,  1.8487e+00],\n",
       "         [-1.5386e+00,  1.8487e+00]],\n",
       "\n",
       "        [[-1.5386e+00,  1.8487e+00],\n",
       "         [-1.5386e+00,  1.8487e+00],\n",
       "         [ 1.7984e-03,  1.6052e+00]],\n",
       "\n",
       "        [[-1.5386e+00,  1.8487e+00],\n",
       "         [ 1.7984e-03,  1.6052e+00],\n",
       "         [ 7.3711e-01,  4.8597e-01]],\n",
       "\n",
       "        [[ 1.7984e-03,  1.6052e+00],\n",
       "         [ 7.3711e-01,  4.8597e-01],\n",
       "         [ 1.7211e+00,  1.4553e+00]],\n",
       "\n",
       "        [[ 7.3711e-01,  4.8597e-01],\n",
       "         [ 1.7211e+00,  1.4553e+00],\n",
       "         [ 2.8026e-01, -1.0524e+00]],\n",
       "\n",
       "        [[ 1.7211e+00,  1.4553e+00],\n",
       "         [ 2.8026e-01, -1.0524e+00],\n",
       "         [ 1.7211e+00,  1.4553e+00]],\n",
       "\n",
       "        [[ 2.8026e-01, -1.0524e+00],\n",
       "         [ 1.7211e+00,  1.4553e+00],\n",
       "         [-6.7712e-01,  1.9750e+00]],\n",
       "\n",
       "        [[-1.5386e+00,  1.8487e+00],\n",
       "         [-1.5386e+00,  1.8487e+00],\n",
       "         [-1.5386e+00,  1.8487e+00]],\n",
       "\n",
       "        [[-1.5386e+00,  1.8487e+00],\n",
       "         [-1.5386e+00,  1.8487e+00],\n",
       "         [-6.7712e-01,  1.9750e+00]],\n",
       "\n",
       "        [[-1.5386e+00,  1.8487e+00],\n",
       "         [-6.7712e-01,  1.9750e+00],\n",
       "         [ 2.8026e-01, -1.0524e+00]],\n",
       "\n",
       "        [[-6.7712e-01,  1.9750e+00],\n",
       "         [ 2.8026e-01, -1.0524e+00],\n",
       "         [-6.7712e-01,  1.9750e+00]],\n",
       "\n",
       "        [[-1.5386e+00,  1.8487e+00],\n",
       "         [-1.5386e+00,  1.8487e+00],\n",
       "         [-1.5386e+00,  1.8487e+00]],\n",
       "\n",
       "        [[-1.5386e+00,  1.8487e+00],\n",
       "         [-1.5386e+00,  1.8487e+00],\n",
       "         [ 1.7211e+00,  1.4553e+00]],\n",
       "\n",
       "        [[-1.5386e+00,  1.8487e+00],\n",
       "         [ 1.7211e+00,  1.4553e+00],\n",
       "         [-4.7952e-01, -1.0606e+00]],\n",
       "\n",
       "        [[ 1.7211e+00,  1.4553e+00],\n",
       "         [-4.7952e-01, -1.0606e+00],\n",
       "         [-6.7712e-01,  1.9750e+00]],\n",
       "\n",
       "        [[-4.7952e-01, -1.0606e+00],\n",
       "         [-6.7712e-01,  1.9750e+00],\n",
       "         [ 6.4264e-01,  7.5298e-01]],\n",
       "\n",
       "        [[-6.7712e-01,  1.9750e+00],\n",
       "         [ 6.4264e-01,  7.5298e-01],\n",
       "         [ 1.6538e+00, -1.6568e+00]],\n",
       "\n",
       "        [[ 6.4264e-01,  7.5298e-01],\n",
       "         [ 1.6538e+00, -1.6568e+00],\n",
       "         [ 7.3711e-01,  4.8597e-01]],\n",
       "\n",
       "        [[ 1.6538e+00, -1.6568e+00],\n",
       "         [ 7.3711e-01,  4.8597e-01],\n",
       "         [ 7.3711e-01,  4.8597e-01]],\n",
       "\n",
       "        [[ 7.3711e-01,  4.8597e-01],\n",
       "         [ 7.3711e-01,  4.8597e-01],\n",
       "         [-6.7712e-01,  1.9750e+00]],\n",
       "\n",
       "        [[-1.5386e+00,  1.8487e+00],\n",
       "         [-1.5386e+00,  1.8487e+00],\n",
       "         [-1.5386e+00,  1.8487e+00]],\n",
       "\n",
       "        [[-1.5386e+00,  1.8487e+00],\n",
       "         [-1.5386e+00,  1.8487e+00],\n",
       "         [-4.7952e-01, -1.0606e+00]],\n",
       "\n",
       "        [[-1.5386e+00,  1.8487e+00],\n",
       "         [-4.7952e-01, -1.0606e+00],\n",
       "         [ 1.7984e-03,  1.6052e+00]],\n",
       "\n",
       "        [[-4.7952e-01, -1.0606e+00],\n",
       "         [ 1.7984e-03,  1.6052e+00],\n",
       "         [ 9.0456e-01,  6.3669e-01]],\n",
       "\n",
       "        [[ 1.7984e-03,  1.6052e+00],\n",
       "         [ 9.0456e-01,  6.3669e-01],\n",
       "         [ 7.0645e-01, -6.2030e-01]],\n",
       "\n",
       "        [[ 9.0456e-01,  6.3669e-01],\n",
       "         [ 7.0645e-01, -6.2030e-01],\n",
       "         [ 1.7211e+00,  1.4553e+00]],\n",
       "\n",
       "        [[ 7.0645e-01, -6.2030e-01],\n",
       "         [ 1.7211e+00,  1.4553e+00],\n",
       "         [-6.7712e-01,  1.9750e+00]]])"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(6, 2, 1)"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embeddings.stride()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# implementing the hidden layer\n",
    "\n",
    "W1 = torch.randn((6, 100)) # the number of inputs to this layer will be 3 x 2 = 6, because we have two dimensional embeddings and we have three of them; let's do 100 neurons\n",
    "b1 = torch.randn(100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[-1.5386e+00,  1.8487e+00],\n",
       "         [-1.5386e+00,  1.8487e+00],\n",
       "         [-1.5386e+00,  1.8487e+00],\n",
       "         [ 1.6538e+00, -1.6568e+00],\n",
       "         [ 2.2373e-02, -1.1911e-01],\n",
       "         [-1.5386e+00,  1.8487e+00],\n",
       "         [-1.5386e+00,  1.8487e+00],\n",
       "         [-1.5386e+00,  1.8487e+00],\n",
       "         [ 1.7984e-03,  1.6052e+00],\n",
       "         [ 7.3711e-01,  4.8597e-01],\n",
       "         [ 1.7211e+00,  1.4553e+00],\n",
       "         [ 2.8026e-01, -1.0524e+00],\n",
       "         [-1.5386e+00,  1.8487e+00],\n",
       "         [-1.5386e+00,  1.8487e+00],\n",
       "         [-1.5386e+00,  1.8487e+00],\n",
       "         [-6.7712e-01,  1.9750e+00],\n",
       "         [-1.5386e+00,  1.8487e+00],\n",
       "         [-1.5386e+00,  1.8487e+00],\n",
       "         [-1.5386e+00,  1.8487e+00],\n",
       "         [ 1.7211e+00,  1.4553e+00],\n",
       "         [-4.7952e-01, -1.0606e+00],\n",
       "         [-6.7712e-01,  1.9750e+00],\n",
       "         [ 6.4264e-01,  7.5298e-01],\n",
       "         [ 1.6538e+00, -1.6568e+00],\n",
       "         [ 7.3711e-01,  4.8597e-01],\n",
       "         [-1.5386e+00,  1.8487e+00],\n",
       "         [-1.5386e+00,  1.8487e+00],\n",
       "         [-1.5386e+00,  1.8487e+00],\n",
       "         [-4.7952e-01, -1.0606e+00],\n",
       "         [ 1.7984e-03,  1.6052e+00],\n",
       "         [ 9.0456e-01,  6.3669e-01],\n",
       "         [ 7.0645e-01, -6.2030e-01]]),\n",
       " tensor([[-1.5386e+00,  1.8487e+00],\n",
       "         [-1.5386e+00,  1.8487e+00],\n",
       "         [ 1.6538e+00, -1.6568e+00],\n",
       "         [ 2.2373e-02, -1.1911e-01],\n",
       "         [ 2.2373e-02, -1.1911e-01],\n",
       "         [-1.5386e+00,  1.8487e+00],\n",
       "         [-1.5386e+00,  1.8487e+00],\n",
       "         [ 1.7984e-03,  1.6052e+00],\n",
       "         [ 7.3711e-01,  4.8597e-01],\n",
       "         [ 1.7211e+00,  1.4553e+00],\n",
       "         [ 2.8026e-01, -1.0524e+00],\n",
       "         [ 1.7211e+00,  1.4553e+00],\n",
       "         [-1.5386e+00,  1.8487e+00],\n",
       "         [-1.5386e+00,  1.8487e+00],\n",
       "         [-6.7712e-01,  1.9750e+00],\n",
       "         [ 2.8026e-01, -1.0524e+00],\n",
       "         [-1.5386e+00,  1.8487e+00],\n",
       "         [-1.5386e+00,  1.8487e+00],\n",
       "         [ 1.7211e+00,  1.4553e+00],\n",
       "         [-4.7952e-01, -1.0606e+00],\n",
       "         [-6.7712e-01,  1.9750e+00],\n",
       "         [ 6.4264e-01,  7.5298e-01],\n",
       "         [ 1.6538e+00, -1.6568e+00],\n",
       "         [ 7.3711e-01,  4.8597e-01],\n",
       "         [ 7.3711e-01,  4.8597e-01],\n",
       "         [-1.5386e+00,  1.8487e+00],\n",
       "         [-1.5386e+00,  1.8487e+00],\n",
       "         [-4.7952e-01, -1.0606e+00],\n",
       "         [ 1.7984e-03,  1.6052e+00],\n",
       "         [ 9.0456e-01,  6.3669e-01],\n",
       "         [ 7.0645e-01, -6.2030e-01],\n",
       "         [ 1.7211e+00,  1.4553e+00]]),\n",
       " tensor([[-1.5386e+00,  1.8487e+00],\n",
       "         [ 1.6538e+00, -1.6568e+00],\n",
       "         [ 2.2373e-02, -1.1911e-01],\n",
       "         [ 2.2373e-02, -1.1911e-01],\n",
       "         [-6.7712e-01,  1.9750e+00],\n",
       "         [-1.5386e+00,  1.8487e+00],\n",
       "         [ 1.7984e-03,  1.6052e+00],\n",
       "         [ 7.3711e-01,  4.8597e-01],\n",
       "         [ 1.7211e+00,  1.4553e+00],\n",
       "         [ 2.8026e-01, -1.0524e+00],\n",
       "         [ 1.7211e+00,  1.4553e+00],\n",
       "         [-6.7712e-01,  1.9750e+00],\n",
       "         [-1.5386e+00,  1.8487e+00],\n",
       "         [-6.7712e-01,  1.9750e+00],\n",
       "         [ 2.8026e-01, -1.0524e+00],\n",
       "         [-6.7712e-01,  1.9750e+00],\n",
       "         [-1.5386e+00,  1.8487e+00],\n",
       "         [ 1.7211e+00,  1.4553e+00],\n",
       "         [-4.7952e-01, -1.0606e+00],\n",
       "         [-6.7712e-01,  1.9750e+00],\n",
       "         [ 6.4264e-01,  7.5298e-01],\n",
       "         [ 1.6538e+00, -1.6568e+00],\n",
       "         [ 7.3711e-01,  4.8597e-01],\n",
       "         [ 7.3711e-01,  4.8597e-01],\n",
       "         [-6.7712e-01,  1.9750e+00],\n",
       "         [-1.5386e+00,  1.8487e+00],\n",
       "         [-4.7952e-01, -1.0606e+00],\n",
       "         [ 1.7984e-03,  1.6052e+00],\n",
       "         [ 9.0456e-01,  6.3669e-01],\n",
       "         [ 7.0645e-01, -6.2030e-01],\n",
       "         [ 1.7211e+00,  1.4553e+00],\n",
       "         [-6.7712e-01,  1.9750e+00]]))"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# we can't do embeddings @ W1 + b1 because the embeddings are stacked up and tensor dimensions aren't fit with each other\n",
    "# there are usually many ways to implement this, some are faster, some are shorter etc. torch is large\n",
    "\n",
    "embeddings[:, 0, :], embeddings[:, 1, :], embeddings[:, 2, :]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([32, 6])"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.cat([embeddings[:, 0, :], embeddings[:, 1, :], embeddings[:, 2, :]], 1).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([32, 6])"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.cat(torch.unbind(embeddings, 1), 1).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([ 0,  1,  2,  3,  4,  5,  6,  7,  8,  9, 10, 11, 12, 13, 14, 15, 16, 17])"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a = torch.arange(18)\n",
    "a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([ 0,  1,  2,  3,  4,  5,  6,  7,  8,  9, 10, 11, 12, 13, 14, 15, 16, 17])"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# .view is extremely efficient\n",
    "b = a.view(3, 3, 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_408443/3076968323.py:1: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()\n",
      "  b.storage()\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       " 0\n",
       " 1\n",
       " 2\n",
       " 3\n",
       " 4\n",
       " 5\n",
       " 6\n",
       " 7\n",
       " 8\n",
       " 9\n",
       " 10\n",
       " 11\n",
       " 12\n",
       " 13\n",
       " 14\n",
       " 15\n",
       " 16\n",
       " 17\n",
       "[torch.storage.TypedStorage(dtype=torch.int64, device=cpu) of size 18]"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "b.storage()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-1.5386e+00,  1.8487e+00, -1.5386e+00,  1.8487e+00, -1.5386e+00,\n",
       "          1.8487e+00],\n",
       "        [-1.5386e+00,  1.8487e+00, -1.5386e+00,  1.8487e+00,  1.6538e+00,\n",
       "         -1.6568e+00],\n",
       "        [-1.5386e+00,  1.8487e+00,  1.6538e+00, -1.6568e+00,  2.2373e-02,\n",
       "         -1.1911e-01],\n",
       "        [ 1.6538e+00, -1.6568e+00,  2.2373e-02, -1.1911e-01,  2.2373e-02,\n",
       "         -1.1911e-01],\n",
       "        [ 2.2373e-02, -1.1911e-01,  2.2373e-02, -1.1911e-01, -6.7712e-01,\n",
       "          1.9750e+00],\n",
       "        [-1.5386e+00,  1.8487e+00, -1.5386e+00,  1.8487e+00, -1.5386e+00,\n",
       "          1.8487e+00],\n",
       "        [-1.5386e+00,  1.8487e+00, -1.5386e+00,  1.8487e+00,  1.7984e-03,\n",
       "          1.6052e+00],\n",
       "        [-1.5386e+00,  1.8487e+00,  1.7984e-03,  1.6052e+00,  7.3711e-01,\n",
       "          4.8597e-01],\n",
       "        [ 1.7984e-03,  1.6052e+00,  7.3711e-01,  4.8597e-01,  1.7211e+00,\n",
       "          1.4553e+00],\n",
       "        [ 7.3711e-01,  4.8597e-01,  1.7211e+00,  1.4553e+00,  2.8026e-01,\n",
       "         -1.0524e+00],\n",
       "        [ 1.7211e+00,  1.4553e+00,  2.8026e-01, -1.0524e+00,  1.7211e+00,\n",
       "          1.4553e+00],\n",
       "        [ 2.8026e-01, -1.0524e+00,  1.7211e+00,  1.4553e+00, -6.7712e-01,\n",
       "          1.9750e+00],\n",
       "        [-1.5386e+00,  1.8487e+00, -1.5386e+00,  1.8487e+00, -1.5386e+00,\n",
       "          1.8487e+00],\n",
       "        [-1.5386e+00,  1.8487e+00, -1.5386e+00,  1.8487e+00, -6.7712e-01,\n",
       "          1.9750e+00],\n",
       "        [-1.5386e+00,  1.8487e+00, -6.7712e-01,  1.9750e+00,  2.8026e-01,\n",
       "         -1.0524e+00],\n",
       "        [-6.7712e-01,  1.9750e+00,  2.8026e-01, -1.0524e+00, -6.7712e-01,\n",
       "          1.9750e+00],\n",
       "        [-1.5386e+00,  1.8487e+00, -1.5386e+00,  1.8487e+00, -1.5386e+00,\n",
       "          1.8487e+00],\n",
       "        [-1.5386e+00,  1.8487e+00, -1.5386e+00,  1.8487e+00,  1.7211e+00,\n",
       "          1.4553e+00],\n",
       "        [-1.5386e+00,  1.8487e+00,  1.7211e+00,  1.4553e+00, -4.7952e-01,\n",
       "         -1.0606e+00],\n",
       "        [ 1.7211e+00,  1.4553e+00, -4.7952e-01, -1.0606e+00, -6.7712e-01,\n",
       "          1.9750e+00],\n",
       "        [-4.7952e-01, -1.0606e+00, -6.7712e-01,  1.9750e+00,  6.4264e-01,\n",
       "          7.5298e-01],\n",
       "        [-6.7712e-01,  1.9750e+00,  6.4264e-01,  7.5298e-01,  1.6538e+00,\n",
       "         -1.6568e+00],\n",
       "        [ 6.4264e-01,  7.5298e-01,  1.6538e+00, -1.6568e+00,  7.3711e-01,\n",
       "          4.8597e-01],\n",
       "        [ 1.6538e+00, -1.6568e+00,  7.3711e-01,  4.8597e-01,  7.3711e-01,\n",
       "          4.8597e-01],\n",
       "        [ 7.3711e-01,  4.8597e-01,  7.3711e-01,  4.8597e-01, -6.7712e-01,\n",
       "          1.9750e+00],\n",
       "        [-1.5386e+00,  1.8487e+00, -1.5386e+00,  1.8487e+00, -1.5386e+00,\n",
       "          1.8487e+00],\n",
       "        [-1.5386e+00,  1.8487e+00, -1.5386e+00,  1.8487e+00, -4.7952e-01,\n",
       "         -1.0606e+00],\n",
       "        [-1.5386e+00,  1.8487e+00, -4.7952e-01, -1.0606e+00,  1.7984e-03,\n",
       "          1.6052e+00],\n",
       "        [-4.7952e-01, -1.0606e+00,  1.7984e-03,  1.6052e+00,  9.0456e-01,\n",
       "          6.3669e-01],\n",
       "        [ 1.7984e-03,  1.6052e+00,  9.0456e-01,  6.3669e-01,  7.0645e-01,\n",
       "         -6.2030e-01],\n",
       "        [ 9.0456e-01,  6.3669e-01,  7.0645e-01, -6.2030e-01,  1.7211e+00,\n",
       "          1.4553e+00],\n",
       "        [ 7.0645e-01, -6.2030e-01,  1.7211e+00,  1.4553e+00, -6.7712e-01,\n",
       "          1.9750e+00]])"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embeddings.view(32, 6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embeddings.view(32, 6).shape == torch.cat(torch.unbind(embeddings, 1), 1).shape == torch.cat([embeddings[:, 0, :], embeddings[:, 1, :], embeddings[:, 2, :]], 1).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([32, 6])"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# using -1, so we won't hardcode numbers, when we do -1, torch will infer what this should be\n",
    "h = embeddings.view(-1, 6)\n",
    "h.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([32, 100])\n",
      "torch.Size([100])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([[-0.8156, -0.5897,  0.9998,  ..., -0.4826,  0.8758, -0.9777],\n",
       "        [-0.9547, -0.7934, -0.9582,  ..., -0.9687, -0.9975,  0.9994],\n",
       "        [-0.9855,  0.2595,  0.1929,  ...,  0.9708,  0.8989,  0.9980],\n",
       "        ...,\n",
       "        [-0.9946, -0.0194, -0.4549,  ..., -0.6456, -0.9958, -0.1482],\n",
       "        [-0.9993, -0.9951, -0.9984,  ..., -0.7173, -0.9204, -1.0000],\n",
       "        [ 0.8983,  0.9852,  0.9880,  ..., -0.9917,  0.7021, -1.0000]])"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "W1 = torch.randn((6, 100)) # the number of inputs to this layer will be 3 x 2 = 6, because we have two dimensional embeddings and we have three of them; let's do 100 neurons\n",
    "b1 = torch.randn(100)\n",
    "h = torch.tanh(embeddings.view(-1, 6) @ W1 + b1) # bias will be added to all elements in the rows\n",
    "print(h.shape)\n",
    "print(b1.shape)\n",
    "h"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[  4.8352,  -0.8113,  -5.2077,  -1.6692,  -8.6118,  -4.3560,  -4.4816,\n",
       "          -2.5404, -10.3583,  -9.4154,   3.7328, -13.4735,  -9.5245,   4.2486,\n",
       "          12.7284,   3.8295,  -8.1381,  12.6194,   6.0599,  -7.1012, -13.1742,\n",
       "           3.2031,   6.8065,   9.9618,  -0.3751,  -3.4676,   7.2946],\n",
       "        [ -4.3128,   7.1014,   4.7680,   2.1870, -18.6293,   4.9823,   5.0163,\n",
       "          -9.5518,   3.0199,   7.8746,   6.1167,   4.2357, -10.2116,  10.9785,\n",
       "          -6.8028, -19.0346,  -5.0495,  -0.9637,   3.0904, -11.2131,  -0.3035,\n",
       "          -9.0335,  -1.9587,   5.8284, -14.2298,  -2.4411,  -2.5042],\n",
       "        [ -1.6455,  -6.7552,   9.2235,  -7.3221,  -6.0991,  -1.8336,   7.8402,\n",
       "           5.8387,  -3.9031,  -5.6698,   2.5161,  -6.4892,  27.3049,   5.4840,\n",
       "          -1.3790,  -6.2014,   6.2420,   4.1894, -12.8097, -11.8332,   1.9206,\n",
       "           5.8368,  -9.8765,   2.3276,  11.4135,   5.3316,   3.8546]])"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# let's create the final layer\n",
    "\n",
    "W2 = torch.randn((100, 27)) # the input now will be 100 because the hidden layer has 100 outputs, the output number of these neurons will be 27 because we have 27 possible characters\n",
    "b2 = torch.randn(27) # biases will be 27 as well\n",
    "\n",
    "logits = h @ W2 + b2\n",
    "logits[:3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([32, 27])"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "logits.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1.2587e+02, 4.4426e-01, 5.4744e-03, 1.8839e-01, 1.8194e-04, 1.2829e-02,\n",
       "         1.1315e-02, 7.8836e-02, 3.1729e-05, 8.1457e-05, 4.1796e+01, 1.4077e-06,\n",
       "         7.3041e-05, 7.0004e+01, 3.3720e+05, 4.6040e+01, 2.9218e-04, 3.0237e+05,\n",
       "         4.2831e+02, 8.2409e-04, 1.8991e-06, 2.4608e+01, 9.0373e+02, 2.1200e+04,\n",
       "         6.8721e-01, 3.1192e-02, 1.4724e+03],\n",
       "        [1.3396e-02, 1.2137e+03, 1.1769e+02, 8.9081e+00, 8.1173e-09, 1.4581e+02,\n",
       "         1.5084e+02, 7.1071e-05, 2.0489e+01, 2.6296e+03, 4.5338e+02, 6.9110e+01,\n",
       "         3.6743e-05, 5.8602e+04, 1.1106e-03, 5.4122e-09, 6.4127e-03, 3.8150e-01,\n",
       "         2.1985e+01, 1.3496e-05, 7.3824e-01, 1.1934e-04, 1.4105e-01, 3.3980e+02,\n",
       "         6.6083e-07, 8.7068e-02, 8.1743e-02],\n",
       "        [1.9292e-01, 1.1648e-03, 1.0133e+04, 6.6079e-04, 2.2448e-03, 1.5984e-01,\n",
       "         2.5406e+03, 3.4333e+02, 2.0178e-02, 3.4485e-03, 1.2380e+01, 1.5198e-03,\n",
       "         7.2174e+11, 2.4080e+02, 2.5184e-01, 2.0265e-03, 5.1387e+02, 6.5983e+01,\n",
       "         2.7340e-06, 7.2597e-06, 6.8249e+00, 3.4269e+02, 5.1366e-05, 1.0253e+01,\n",
       "         9.0532e+04, 2.0677e+02, 4.7209e+01]])"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "counts = logits.exp()\n",
    "counts[:3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1.8959e-04, 6.6919e-07, 8.2461e-09, 2.8377e-07, 2.7406e-10, 1.9324e-08,\n",
       "         1.7044e-08, 1.1875e-07, 4.7793e-11, 1.2270e-10, 6.2957e-05, 2.1205e-12,\n",
       "         1.1002e-10, 1.0545e-04, 5.0792e-01, 6.9350e-05, 4.4012e-10, 4.5545e-01,\n",
       "         6.4516e-04, 1.2413e-09, 2.8605e-12, 3.7067e-05, 1.3613e-03, 3.1934e-02,\n",
       "         1.0351e-06, 4.6985e-08, 2.2178e-03],\n",
       "        [2.1005e-07, 1.9031e-02, 1.8454e-03, 1.3968e-04, 1.2728e-13, 2.2864e-03,\n",
       "         2.3653e-03, 1.1144e-09, 3.2128e-04, 4.1233e-02, 7.1090e-03, 1.0837e-03,\n",
       "         5.7614e-10, 9.1889e-01, 1.7415e-08, 8.4865e-14, 1.0055e-07, 5.9819e-06,\n",
       "         3.4473e-04, 2.1163e-10, 1.1576e-05, 1.8713e-09, 2.2116e-06, 5.3281e-03,\n",
       "         1.0362e-11, 1.3652e-06, 1.2817e-06],\n",
       "        [2.6730e-13, 1.6139e-15, 1.4040e-08, 9.1556e-16, 3.1102e-15, 2.2147e-13,\n",
       "         3.5202e-09, 4.7570e-10, 2.7958e-14, 4.7781e-15, 1.7153e-11, 2.1058e-15,\n",
       "         1.0000e+00, 3.3364e-10, 3.4894e-13, 2.8078e-15, 7.1200e-10, 9.1422e-11,\n",
       "         3.7881e-18, 1.0059e-17, 9.4563e-12, 4.7481e-10, 7.1171e-17, 1.4207e-11,\n",
       "         1.2544e-07, 2.8649e-10, 6.5411e-11]])"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prob = counts / counts.sum(1, keepdim=True)\n",
    "prob[:3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(1.0000)"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prob[0].sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([ 5, 13, 13,  1,  0, 15, 12,  9, 22,  9,  1,  0,  1, 22,  1,  0,  9, 19,\n",
       "         1,  2,  5, 12, 12,  1,  0, 19, 15, 16,  8,  9,  1,  0])"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# index into the rows of prob and in each row pluck out the probability assigned to the correct character\n",
    "Y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([1.9324e-08, 9.1889e-01, 3.3364e-10, 4.4633e-01, 5.1886e-05, 6.9350e-05,\n",
       "        8.6384e-10, 9.2564e-02, 7.1187e-09, 9.1351e-05, 1.6712e-09, 8.4133e-08,\n",
       "        6.6919e-07, 4.0736e-04, 1.8228e-07, 1.5693e-07, 1.2270e-10, 2.0883e-09,\n",
       "        1.5495e-14, 2.9964e-08, 2.6122e-10, 8.7837e-08, 6.1697e-01, 3.2713e-01,\n",
       "        6.6076e-06, 1.2413e-09, 2.6503e-07, 5.8911e-08, 1.1901e-06, 2.1394e-05,\n",
       "        2.4917e-10, 3.4066e-08])"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prob[torch.arange(32), Y]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(14.3971)"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loss = -prob[torch.arange(32), Y].log().mean()\n",
    "loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Rewrite"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In machine learning, parameters and hyperparameters are fundamental concepts.\n",
    "\n",
    "    Parameters: Parameters are the internal variables whose values are learned from the training data during the model fitting process. These parameters define the structure and behavior of the model. For example, in a simple linear regression model, parameters would be the coefficients and the intercept. In a neural network, parameters would include the weights and biases of the neurons in each layer.\n",
    "\n",
    "    Hyperparameters: Hyperparameters are external configuration settings that are not learned from the data but are set prior to the training process. They control the learning process and the overall behavior of the model. Examples of hyperparameters include the learning rate, regularization strength, number of hidden layers in a neural network, and the choice of algorithm itself.\n",
    "\n",
    "The key differences between parameters and hyperparameters are:\n",
    "\n",
    "    Parameters are learned during the training process, while hyperparameters are set before the training process.\n",
    "    Parameters are specific to the model and its structure, while hyperparameters are external factors that affect the learning process.\n",
    "\n",
    "Tuning hyperparameters effectively is crucial for optimizing a model's performance. This process often involves experimentation, such as using techniques like grid search or random search to find the best combination of hyperparameters for a given task and dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    C, W1, b1, W2, and b2 are tensors. These tensors represent the weights and biases of a neural network. They are learned during the training process and directly affect the behavior of the model based on the data. Therefore, they are parameters.\n",
    "\n",
    "    Hyperparameters, on the other hand, are external settings that control the learning process itself, such as the learning rate, regularization strength, or the structure of the network (e.g., number of layers, number of neurons per layer). These hyperparameters are not learned from the data but are set by the user before the training process begins."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([32, 3]), torch.Size([32]))"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X.shape, Y.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "g = torch.Generator().manual_seed(2147483647) \n",
    "C = torch.randn((27, 2), generator=g)\n",
    "W1 = torch.randn((6, 100), generator=g)\n",
    "b1 = torch.randn(100, generator=g)\n",
    "W2 = torch.randn((100, 27), generator=g)\n",
    "b2 = torch.randn((27), generator=g)\n",
    "parameters = [C, W1, b1, W2, b2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3481"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sum(p.nelement() for p in parameters) # number of parameters in total"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(17.7697)"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# forward pass\n",
    "embeddings = C[X] # (32, 3, 2)\n",
    "h = torch.tanh(embeddings.view(-1, 6) @ W1 + b1) # (32, 100) \n",
    "logits = h @ W2 + b2 # (32, 27)\n",
    "\n",
    "# embeddings = C[X] performs the first step of the forward pass, where the input X is used to index into the tensor C to obtain the corresponding embeddings. This step transforms the input into a higher-dimensional representation based on the embedding matrix C.\n",
    "# h = torch.tanh(embeddings.view(-1, 6) @ W1 + b1) represents the propagation of the transformed input through the first hidden layer of the neural network. It computes the activations of the neurons in the hidden layer using the hyperbolic tangent (tanh) activation function.\n",
    "# logits = h @ W2 + b2 calculates the final logits by propagating the activations from the hidden layer through the output layer of the neural network.\n",
    "# These three lines together constitute the forward pass of the neural network, where input data is processed through the layers of the network to generate predictions.\n",
    "\n",
    "# softmax\n",
    "counts = logits.exp()\n",
    "prob = counts / counts.sum(1, keepdim=True)\n",
    "\n",
    "# loss\n",
    "loss = -prob[torch.arange(32), Y].log().mean()\n",
    "loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(17.7697)"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# let's make this even more respectable\n",
    "# what we did in the cell above is classification and for this there's cross_entropy function in pytorch.\n",
    "# we can simply call F.cross_entropy, pass in the logits and the array of targets Y, and this calculates the exact same loss\n",
    "\n",
    "F.cross_entropy(logits, Y)\n",
    "\n",
    "# when you use f.cross entropy by torch will not actually create all those intermediate tensors from the above cell, because these are all new tensors in memory and all this is fairly inefficient to run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([3.7835e-44, 4.9787e-02, 1.0000e+00,        inf])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([0., 0., 0., nan])"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# also cross_entropy is very well behaved\n",
    "# bad behavior down here, when one logit is very positive, you get a nan\n",
    "\n",
    "logits = torch.tensor([-100, -3, 0, 100])\n",
    "counts = logits.exp()\n",
    "probs = counts / counts.sum()\n",
    "print(counts)\n",
    "probs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0.0000e+00, 1.4013e-45, 3.7835e-44, 1.0000e+00])"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# the way pytorch solved this is by offsetting the problematic number by itself.\n",
    "\n",
    "logits = torch.tensor([-100, -3, 0, 100]) - 100\n",
    "counts = logits.exp()\n",
    "probs = counts / counts.sum()\n",
    "probs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### implementing the training loop, overfitting one batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "for p in parameters:\n",
    "    p.requires_grad = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "17.76971435546875\n",
      "13.656401634216309\n",
      "11.298768997192383\n",
      "9.452455520629883\n",
      "7.984262466430664\n",
      "6.891321182250977\n",
      "6.100014686584473\n",
      "5.452036380767822\n",
      "4.898151874542236\n",
      "4.414664268493652\n"
     ]
    }
   ],
   "source": [
    "for _ in range(10):\n",
    "    # forward pass\n",
    "    embeddings = C[X] # (32, 3, 2)\n",
    "    h = torch.tanh(embeddings.view(-1, 6) @ W1 + b1) # (32, 100) \n",
    "    logits = h @ W2 + b2 # (32, 27)\n",
    "\n",
    "    loss = F.cross_entropy(logits, Y)\n",
    "\n",
    "    print(loss.item())\n",
    "\n",
    "    # backward pass\n",
    "    for p in parameters:\n",
    "        p.grad = None\n",
    "    loss.backward()\n",
    "\n",
    "    # update\n",
    "    for p in parameters:\n",
    "        p.data += -0.1 * p.grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.25609633326530457\n"
     ]
    }
   ],
   "source": [
    "# run for a thousand times we get a very very low loss and that means that we're making very good predictions\n",
    "# now the reason that this is so straightforward right now is because we're only overfitting 32 examples,\n",
    "# so we only have 32 examples of the first 5 words and therefore it's very easy to make this neural net fit only these two 32\n",
    "# because we have 3400 parameters and only 32 examples so we're doing what's called overfitting a single batch of the data and getting a very low loss and good predictions, but that's just because we have so many parameters for so few examples so it's easy to make this be very low\n",
    "\n",
    "for _ in range(1000):\n",
    "    # forward pass\n",
    "    embeddings = C[X] # (32, 3, 2)\n",
    "    h = torch.tanh(embeddings.view(-1, 6) @ W1 + b1) # (32, 100) \n",
    "    logits = h @ W2 + b2 # (32, 27)\n",
    "\n",
    "    loss = F.cross_entropy(logits, Y)\n",
    "\n",
    "    # backward pass\n",
    "    for p in parameters:\n",
    "        p.grad = None\n",
    "    loss.backward()\n",
    "\n",
    "    # update\n",
    "    for p in parameters:\n",
    "        p.data += -0.1 * p.grad\n",
    "\n",
    "print(loss.item())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Moving tensors to the GPU\n",
    "\n",
    "g = torch.Generator().manual_seed(2147483647) \n",
    "C = torch.randn((27, 2), generator=g)\n",
    "W1 = torch.randn((6, 100), generator=g)\n",
    "b1 = torch.randn(100, generator=g)\n",
    "W2 = torch.randn((100, 27), generator=g)\n",
    "b2 = torch.randn((27), generator=g)\n",
    "\n",
    "# Define the parameters list\n",
    "parameters = [C, W1, b1, W2, b2]\n",
    "for p in parameters:\n",
    "    p.requires_grad = True\n",
    "\n",
    "# Move tensors to GPU\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "C = C.to(device)\n",
    "W1 = W1.to(device)\n",
    "b1 = b1.to(device)\n",
    "W2 = W2.to(device)\n",
    "b2 = b2.to(device)\n",
    "X = X.to(device)\n",
    "Y = Y.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "17.769710540771484\n"
     ]
    }
   ],
   "source": [
    "# Training loop\n",
    "for _ in range(1000):\n",
    "    with torch.cuda.device(device):\n",
    "        # Forward pass\n",
    "        embeddings = C[X]  # (32, 3, 2)\n",
    "        h = torch.tanh(embeddings.view(-1, 6) @ W1 + b1)  # (32, 100)\n",
    "        logits = h @ W2 + b2  # (32, 27)\n",
    "\n",
    "        loss = F.cross_entropy(logits, Y)\n",
    "\n",
    "        # Backward pass\n",
    "        for p in parameters:\n",
    "            p.grad = None\n",
    "        loss.backward()\n",
    "\n",
    "        # Update\n",
    "        for p in parameters:\n",
    "            p.data += -0.1 * p.grad\n",
    "\n",
    "print(loss.item())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.return_types.max(\n",
       "values=tensor([21.6638, 26.2643, 25.9028, 20.5271, 19.7233, 21.6638, 27.8875, 18.4312,\n",
       "        17.8855, 12.3067, 16.3167, 14.7824, 21.6638, 20.7853, 14.9944, 17.8890,\n",
       "        21.6638, 21.3690, 18.6206, 17.2354, 11.1006, 17.4859, 15.9277, 13.6605,\n",
       "        15.6246, 21.6638, 21.0790, 14.4026, 14.7089, 23.1252, 15.9744, 24.1714],\n",
       "       device='cuda:0', grad_fn=<MaxBackward0>),\n",
       "indices=tensor([10, 10,  9, 14, 10, 10, 10, 26,  9, 21,  2, 14, 10, 10, 10, 22, 10, 10,\n",
       "        10, 10, 14, 10, 26, 23, 20, 10, 10, 10, 23,  9, 14, 10],\n",
       "       device='cuda:0'))"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "logits.max(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([ 5, 13, 13,  1,  0, 15, 12,  9, 22,  9,  1,  0,  1, 22,  1,  0,  9, 19,\n",
       "         1,  2,  5, 12, 12,  1,  0, 19, 15, 16,  8,  9,  1,  0],\n",
       "       device='cuda:0')"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "# build the dataset\n",
    "\n",
    "block_size = 3 # context length: how many characters do we take to predict the next one\n",
    "X, Y = [], []\n",
    "\n",
    "for w in words:\n",
    "    #print(w)\n",
    "    context = [0] * block_size\n",
    "\n",
    "    for ch in w + '.':\n",
    "        ix = stoi[ch]\n",
    "        X.append(context)\n",
    "        Y.append(ix)\n",
    "        #print(''.join(itos[i] for i in context), '---->', itos[ix])\n",
    "        context = context[1:] + [ix] # crop and append\n",
    "\n",
    "X = torch.tensor(X)\n",
    "Y = torch.tensor(Y)\n",
    "# this is kind of like a rolling window of context"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([228146, 3]), torch.int64, torch.Size([228146]), torch.int64)"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X.shape, X.dtype, Y.shape, Y.dtype"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "C = torch.randn((27, 2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([228146, 3, 2])"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embeddings = C[X]\n",
    "embeddings.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Moving tensors to the GPU\n",
    "\n",
    "g = torch.Generator().manual_seed(2147483647) \n",
    "C = torch.randn((27, 2), generator=g)\n",
    "W1 = torch.randn((6, 100), generator=g)\n",
    "b1 = torch.randn(100, generator=g)\n",
    "W2 = torch.randn((100, 27), generator=g)\n",
    "b2 = torch.randn((27), generator=g)\n",
    "\n",
    "# Define the parameters list\n",
    "parameters = [C, W1, b1, W2, b2]\n",
    "for p in parameters:\n",
    "    p.requires_grad = True\n",
    "\n",
    "# Move tensors to GPU\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "C = C.to(device)\n",
    "W1 = W1.to(device)\n",
    "b1 = b1.to(device)\n",
    "W2 = W2.to(device)\n",
    "b2 = b2.to(device)\n",
    "X = X.to(device)\n",
    "Y = Y.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3481"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sum(p.nelement() for p in parameters) # number of parameters in total"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "19.505231857299805\n",
      "19.505231857299805\n",
      "19.505231857299805\n",
      "19.505231857299805\n",
      "19.505231857299805\n",
      "19.505231857299805\n",
      "19.505231857299805\n",
      "19.505231857299805\n",
      "19.505231857299805\n",
      "19.505231857299805\n"
     ]
    }
   ],
   "source": [
    "# Training loop\n",
    "for _ in range(10):\n",
    "    with torch.cuda.device(device):\n",
    "        # Forward pass\n",
    "        embeddings = C[X]  # (32, 3, 2)\n",
    "        h = torch.tanh(embeddings.view(-1, 6) @ W1 + b1)  # (32, 100)\n",
    "        logits = h @ W2 + b2  # (32, 27)\n",
    "\n",
    "        loss = F.cross_entropy(logits, Y)\n",
    "        print(loss.item())\n",
    "\n",
    "        # Backward pass\n",
    "        for p in parameters:\n",
    "            p.grad = None\n",
    "        loss.backward()\n",
    "\n",
    "        # Update\n",
    "        for p in parameters:\n",
    "            p.data += -0.1 * p.grad\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mlp-crlRM3oL-py3.10",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
